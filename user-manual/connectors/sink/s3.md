---
title: Amazon S3 Sink
category: 6321d3a8dc727406c2977082
---

# Amazon S3 Sink

This document provides a brief introduction of the Amazon S3 Sink. It's also designed to guide you through the 
process of running an Amazon S3 Sink Connector.

## Introduction

The Amazon S3(Simple Storage Service) Sink Connector help you **convert received CloudEvents into JSON format and upload** 
them to AWS S3. To enable the amazon S3 service, you should configure your **AWS region and bucket name**. When CloudEvents
arrive, S3 Sink will **cache** them in local files. Then S3 Sink will upload local files according to the **configured upload 
strategy**. Files uploaded to AWS S3 will be **partitioned by time**. S3 Sink supports **`HOURLY` and `DAILY`** partitioning. 

## Features

- **Upload scheduled interval**: S3 Sink supports **scheduled periodic check** for closing and uploading files to S3. When 
the **time interval** reaches the threshold, the file will be directly closed and uploaded, regardless of whether the file 
is full. The interval defaults to **60 seconds**.
- **Flush size**: When file reaches **flush size**, it will be uploaded automatically. The flush size defaults to **1000**. 
- **Partition**: CloudEvents uploaded by S3 Sink are stored in S3 **partitioned**. S3 Sink create storage path in S3 according
to current time. Time-based partitioning options are daily or hourly. 

## Limitations

- **valid schema**: Data S3 Sink received must conform **[CloudEvents Schema Registry][ce-schema]**. 

## IAM policy for Amazon S3 Sink

- s3: PutObject
- s3: GetObject
- s3: ListAllMyBuckets
- s3: ListBucket
- s3: GetBucketLocation
- s3: AbortMultipartUpload
- s3: ListMultipartUploadParts
- s3: ListBucketMultipartUploads

## Quick Start

This quick start will guide you through the process of running an Amazon S3 Sink connector.

### Prerequisites

- A container runtime (i.e., docker).
- An Amazon [S3 bucket][s3-bucket].
- A Properly settled [IAM] policy for your AWS user account.
- An AWS account configured with [Access Keys][access-keys].

### Set S3 Sink Configurations

You can specify your configs by either **setting environments variables** or **mounting a config.json to 
`/vance/config/config.json`** when running the connector.

Here is an example of a configuration file for the Amazon SNS Source.

```shell 
$ vim config.json
{
  "v_port": "8080",
  "region": "us-west-2",
  "bucketName": "${bucketName}",
  "flushSize": "1000",
  "scheduledInterval": "60",
  "timeInterval": "HOURLY"
}
```

|  Configs    |  Description    																  |  Example    			  |  Required    |
|  :----:     |  :----:         																  |  :----:     			  |  :----:      |
|  v_port   |  `v_port` specifies the port S3 Sink will listen to  |  "8080"  |  YES  		 |
|       |  the port of http/http endpoints to receive SNS messages					  |  "8080"	                  |  YES         |
|  region     |  `region` describes in which aws region S3 bucket is created. 				  |  "us-west-2"	                  |  YES         |
|  bucketName     |  unique name of S3 bucket.					  |  "aws-s3-notify"	                  |  YES         |
|  flushSize     |  `flushSize` refers to the number of CloudEvents cached to the local file before S3 Sink committing the file. |  "1000"	     |  NO, defaults to 1000         |
|  scheduledInterval     |  `scheduledInterval` refers to the maximum time interval between S3 Sink closing and uploading files which unit is second.  |  "60"	     |  NO, defaults to 60         |
|  timeInterval     |  `timeInterval` refers to the partitioning interval of files have been uploaded to the S3. S3 Sink supports `HOURLY` and `DAILY` time interval. For example, when `timeInterval` is `HOURLY`, files uploaded between 3 pm and 4 pm will be partitioned to one path, while files uploaded after 4 pm will be partitioned to another path. |  "HOURLY"	     |  YES        |

### Set S3 Sink Secrets 

Users should set their sensitive data **Base64 encoded** in a secret file. And **mount that secret file to 
`/vance/secret/secret.json`** when running the connector.

Replace `MY_SECRET` with your sensitive data to get the Base64-based string.

```shell
$ echo -n MY_SECRET | base64
TVlfU0VDUkVU
```

Create a local secret file with the Base64-based string you just got from your secrets.

```shell
$ vim secret.json
{
  "awsAccessKeyID": "TVlfU0VDUkVU",
  "awsSecretAccessKey": "U2VjcmV0QWNjZXNzS2V5"
}
```
| Secrets            | Description                                                          | Example                       |Required|
|:-------------------|:---------------------------------------------------------------------|:------------------------------|:-------|
| awsAccessKeyID     | `awsAccessKeyID` is the Access key ID of your aws credential.        | "BASE64VALUEOFYOURACCESSKEY=" |**YES** |
| awsSecretAccessKey | `awsSecretAccessKey` is the Secret access key of youraws credential. | "BASE64VALUEOFYOURSECRETKEY=" |**YES** |

### Run the Amazon S3 Sink with Docker

Create your `config.json` and `secret.json`, and mount them to specific paths to run the SNS source using following command.

```shell
docker run -v $(pwd)/secret.json:/vance/secret/secret.json -v $(pwd)/config.json:/vance/config/config.json -p 8080:8080 --rm vancehub/sink-aws-s3
```

### Verify the Amazon S3 Sink

To verify the S3 Sink, you should send CloudEvents to address of S3 Sink. Try to use following `curl` command. 

```shell 
[root@iZ8vbhcwtixrzhsn023sghZ ~]# curl -X POST \
> -d '{"specversion":"0.3","id":"b25e2717-a470-45a0-8231-985a99aa9416","type":"com.github.pull.create","source":"https://github.com/cloudevents/spec/pull/123","time":"2019-07-04T17:31:00.000Z","datacontenttype":"application/json","data":{"much":"wow"}}' \
> -H'Content-Type:application/cloudevents+json' \
> http://${host ip}:8080
``` 

To fill the `${host ip}`, You can use your host IP and the port which S3 Sink docker image exposes, or you can use the ip address of container 
where Sink S3 Connector runs.

Try following command to get the ip address of container.
Use `docker ps` to get the Container id of Sink S3 Connector.
```shell 
docker ps
```
Use `docker inspect` to get the ip address.
```shell 
docker inspect [CONTAINER ID]
``` 

After S3 Sink Connector, you can see following logs:

```shell 
[root@iZ8vbhcwtixrzhsn023sghZ ~]# curl -X POST \
> -d '{"specversion":"0.3","id":"b25e2717-a470-45a0-8231-985a99aa9416","type":"com.github.pull.create","source":"https://github.com/cloudevents/spec/pull/123","time":"2019-07-04T17:31:00.000Z","datacontenttype":"application/json","data":{"much":"wow"}}' \
> -H'Content-Type:application/cloudevents+json' \
> http://8.142.113.12:8080
receive CloudEvent success
```

```shell 
[root@iZ8vbhcwtixrzhsn023sghZ s3sink]# docker run -v $(pwd)/secret.json:/vance/secret/secret.json -v $(pwd)/config.json:/vance/config/config.json -p 8080:8080 --rm sink-aws-s3
[09:02:19:001] [INFO] - com.linkall.vance.common.config.ConfigPrinter.printVanceConf(ConfigPrinter.java:10) - vance configs-v_target: v_target
[09:02:19:010] [INFO] - com.linkall.vance.common.config.ConfigPrinter.printVanceConf(ConfigPrinter.java:11) - vance configs-v_port: 8080
[09:02:19:010] [INFO] - com.linkall.vance.common.config.ConfigPrinter.printVanceConf(ConfigPrinter.java:12) - vance configs-v_config_path: /vance/config/config.json
[09:02:19:011] [INFO] - com.linkall.sink.aws.AwsHelper.checkCredentials(AwsHelper.java:14) - ====== Check aws Credential start ======
[09:02:19:018] [INFO] - com.linkall.sink.aws.AwsHelper.checkCredentials(AwsHelper.java:17) - ====== Check aws Credential end ======
[09:02:20:583] [INFO] - com.linkall.vance.core.http.HttpServerImpl.lambda$listen$5(HttpServerImpl.java:127) - HttpServer is listening on port: 8080
[02:39:20:943] [INFO] - com.linkall.sink.aws.S3Sink.lambda$start$0(S3Sink.java:96) - receive a new event, in total: 1
[02:40:14:141] [INFO] - com.linkall.sink.aws.S3Sink.uploadFile(S3Sink.java:162) - [upload file <eventing-0000001> completed
```

Then check your S3 bucket, you can see files uploaded partitioned by time. 

[ce-schema]: https://github.com/cloudevents/spec/blob/main/schemaregistry/spec.md